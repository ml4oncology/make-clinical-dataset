{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import polars as pl\n",
    "from rapidfuzz import fuzz\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from vllm import LLM\n",
    "\n",
    "from make_clinical_dataset.shared.constants import ROOT_DIR\n",
    "from make_clinical_dataset.epic.combine import get_clinic_prior_to_treatment\n",
    "from make_clinical_dataset.epic.util import hash_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE = '2025-03-29'\n",
    "DATA_DIR = f\"{ROOT_DIR}/data/final/data_{DATE}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ED Risk Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take ALL notes within 5 days prior to a treatment session\n",
    "clinic = pl.read_parquet(f'{DATA_DIR}/interim/clinic_visits.parquet')\n",
    "chemo = pl.read_parquet(f'{DATA_DIR}/interim/chemo.parquet')\n",
    "df = get_clinic_prior_to_treatment(clinic, chemo, lookback_window=5, strategy='all')\n",
    "df.write_parquet(f'{DATA_DIR}/interim/subsets/clinic_visits_prior_to_treatment/notes.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ED risk summary from clinical notes by prompting Qwen3-14B\n",
    "# Run ml4o-batch-inference (see https://github.com/ml4oncology/ml4o-batch-inference)\n",
    "# Example of the SLURM script below\n",
    "\"\"\"\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=batch-inference\n",
    "#SBATCH --partition=gpu\n",
    "#SBATCH --account=grantgroup_gpu\n",
    "#SBATCH --time=23:59:59\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --gres=gpu:l40:1\n",
    "#SBATCH --cpus-per-task=8\n",
    "#SBATCH --mem=32G\n",
    "#SBATCH --output=/cluster/home/%u/logs/%j.out\n",
    "#SBATCH --error=/cluster/home/%u/logs/%j.err\n",
    "\n",
    "mkdir -p /cluster/home/$USER/logs\n",
    "\n",
    "# Load Apptainer module\n",
    "module load apptainer\n",
    "\n",
    "# Load the paths\n",
    "source .env\n",
    "\n",
    "# Set up bind paths\n",
    "export APPTAINER_BINDPATH=$APPTAINER_BINDPATH,$MODEL_PATH\n",
    "\n",
    "# Run batch inference script inside the container\n",
    "apptainer exec --nv $IMAGE_PATH python3.10 ~/repos/ml4o-batch-inference/batch_inference.py \\\n",
    "        --data-path $DATA_PATH \\\n",
    "        --output-path $OUTPUT_PATH \\\n",
    "        --prompt-path ~/repos/make-clinical-dataset/epic/prompts/ed_risk_summarizer.txt \\\n",
    "        --model-name Qwen_Qwen3-14B-IQ4_XS.gguf \\\n",
    "        --tokenizer-path $LLM_PATH/Qwen3-14B \\\n",
    "        --max-model-len 5120 \\\n",
    "        --max-num-seqs 42 \\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the generated output\n",
    "OUTPUT_PATH = \"/cluster/projects/gliugroup/work_dir/kevin_he/BatchInferOutput/ed_risk_summary/batch_infer/generated_output\"\n",
    "df = pl.read_parquet(f\"{OUTPUT_PATH}/*.parquet\")\n",
    "notes = pl.read_parquet(f'{DATA_DIR}/interim/subsets/clinic_visits_prior_to_treatment/notes.parquet', columns=['note_id', 'note'])\n",
    "df, notes = df.unique('note_id'), notes.unique('note_id') # take the first if duplicated\n",
    "df = df.join(notes, on='note_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "SECTION_MAP = {\n",
    "    \"=== ACTIVE SYMPTOMS ===\": \"active_symptoms\", \n",
    "    \"=== RECENT COMPLICATIONS / ADVERSE EVENTS ===\": \"recent_complications\",\n",
    "    \"=== RECENT HEALTHCARE UTILIZATION ===\": \"healthcare_utilization\",\n",
    "    \"=== FUNCTIONAL STATUS / DECLINE ===\": \"functional_status\",\n",
    "    \"=== MEDICATION-RELATED RISKS ===\": \"medication_risks\",\n",
    "    \"=== PSYCHOSOCIAL / SUPPORT RISKS ===\": \"psychosocial_risks\",\n",
    "    \"=== CLINICAL UNCERTAINTY / WATCHFUL WAITING ===\": \"clinical_uncertainty\",\n",
    "    \"=== OVERALL ACUITY ASSESSMENT ===\": \"acuity_assessment\",\n",
    "}\n",
    "SECTION_COLS = list(SECTION_MAP.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into individual sections\n",
    "\n",
    "# remove samples where section was not present\n",
    "# for section in SECTION_NAMES:\n",
    "#     mask = ~df['generated_output'].str.contains(section)\n",
    "#     print(f\"Excluding {mask.sum()} ({mask.mean()*100:.2f}%) samples without section {section}\")\n",
    "#     df = df.filter(~mask)\n",
    "\n",
    "data = []\n",
    "for text in tqdm(df['generated_output']):\n",
    "    res = {}\n",
    "\n",
    "    # get all the headers from the generated output\n",
    "    pattern = r\"===\\s*([^=]+)\\s*===\"\n",
    "    matches = list(re.finditer(pattern, text))\n",
    "\n",
    "    for i, match in enumerate(matches):\n",
    "        header = text[match.start():match.end()]\n",
    "\n",
    "        # find the section that matches the header the closest (LLM does make typos unfortunately)\n",
    "        # TODO: fix from source, ensure guided regex with vllm\n",
    "        for section, name in SECTION_MAP.items():\n",
    "            score = fuzz.ratio(header, section)\n",
    "            if score >= 90:\n",
    "                break\n",
    "\n",
    "        # get the content of the section\n",
    "        section_start = match.end()\n",
    "        section_end = matches[i+1].start() if i < len(matches) - 1 else len(text)\n",
    "        content = text[section_start:section_end]\n",
    "\n",
    "        # clean up the content\n",
    "        content = content.strip()\n",
    "\n",
    "        # store in res\n",
    "        res[name] = content\n",
    "        \n",
    "    data.append(res)\n",
    "\n",
    "data = pl.DataFrame(data)\n",
    "\n",
    "# Keep note id and the original generated output\n",
    "data = pl.concat([data, df.select('note_id', 'generated_output')], how='horizontal')\n",
    "\n",
    "# Create text hash for each section\n",
    "data = data.with_columns([pl.col(col).map_elements(hash_text, return_dtype=pl.String).alias(f\"{col}_text_id\") for col in SECTION_COLS])\n",
    "\n",
    "data.write_parquet(f\"{DATA_DIR}/interim/embedding/ed_risk_summary.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-compute the embeddings\n",
    "data = pl.read_parquet(f\"{DATA_DIR}/interim/embedding/ed_risk_summary.parquet\")\n",
    "data = pl.concat([data.select(pl.col(col).alias(\"text\"), pl.col(f\"{col}_text_id\").alias(\"text_id\")) for col in SECTION_COLS]).drop_nulls().unique()\n",
    "\n",
    "version = \"1.0.0\"\n",
    "source_path = f\"{DATA_DIR}/interim/clinic_visits.parquet\"\n",
    "current_timestamp = datetime.now().date()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### PubMedBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f\"{ROOT_DIR}/LLMs/PubMedBERT\"\n",
    "model = SentenceTransformer(model_path)\n",
    "embed_dim = model.get_sentence_embedding_dimension()\n",
    "\n",
    "# metadata values need to be all string\n",
    "metadata = {\n",
    "    \"source\": source_path,\n",
    "    \"model\": model_path,\n",
    "    \"created_at\": str(current_timestamp),\n",
    "    \"embedding_dim\": str(embed_dim),\n",
    "    \"version\": str(version)\n",
    "}\n",
    "\n",
    "# check number of texts that exceed max seq length\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# token_lengths = tokenizer(data['text'].to_list(), add_special_tokens=False, return_length=True)[\"length\"]\n",
    "# print(f\"Number of texts exceeding max seq length: {(pl.Series(token_lengths) > model.max_seq_length).sum()}\")\n",
    "# del tokenizer\n",
    "\n",
    "outputs = model.encode(data['text'], batch_size=64)\n",
    "data = data.with_columns(pl.Series(\"embedding\", outputs))\n",
    "data.write_parquet(f\"{DATA_DIR}/interim/embedding/PubMedBERT/text_embedding_0.parquet\", metadata=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ModernBERT-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f\"{ROOT_DIR}/LLMs/ModernBERT-large\"\n",
    "model = SentenceTransformer(model_path)\n",
    "embed_dim = model.get_sentence_embedding_dimension()\n",
    "\n",
    "metadata = {\n",
    "    \"source\": source_path,\n",
    "    \"model\": model_path,\n",
    "    \"created_at\": current_timestamp,\n",
    "    \"embedding_dim\": embed_dim,\n",
    "    \"version\": version\n",
    "}\n",
    "\n",
    "outputs = model.encode(data['text'], batch_size=64)\n",
    "data = data.with_columns(pl.Series(\"embedding\", outputs))\n",
    "data.write_parquet(f\"{DATA_DIR}/interim/embedding/ModernBERT-large/text_embedding_0.parquet\", metadata=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Try VLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to write in batches or else OOM error occurs\n",
    "\n",
    "model_path = f\"{ROOT_DIR}/LLMs/PubMedBERT\"\n",
    "# model_path = f\"{ROOT_DIR}/LLMs/ModernBERT-large\" # dang, not supported yet\n",
    "model = LLM(model=model_path, task=\"embed\")\n",
    "# max_seq_len = model.model_config.max_model_len\n",
    "# embed_dim = model.model_config.hf_config.hidden_size\n",
    "\n",
    "batch_size = int(1e5)\n",
    "for batch_num, i in enumerate(tqdm(range(0, len(data), batch_size))):\n",
    "    outputs = model.embed(data['text'][i:i+batch_size])\n",
    "    embeddings = [output.outputs.embedding for output in outputs]\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
